{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/vetka/miniconda3/envs/transformers did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('VSCODE_WSL_EXT_LOCATION/up')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import GPTJForCausalLM, LlamaForCausalLM, AutoTokenizer, LlamaTokenizer\n",
    "\n",
    "from autograd_4bit import load_gptj_model_4bit_low_ram, load_llama_model_4bit_low_ram\n",
    "from peft import PeftModel\n",
    "\n",
    "LLAMA_7B_MODEL_PATH = 'decapoda-research/llama-7b-hf'\n",
    "# !wget https://huggingface.co/decapoda-research/llama-7b-hf-int4/resolve/main/llama-7b-4bit.pt\n",
    "LLAMA_7B_4BIT_CHECKPOINT_PATH = './llama-7b-4bit.pt'\n",
    "\n",
    "LLAMA_13B_MODEL_PATH = 'decapoda-research/llama-13b-hf'\n",
    "# !wget https://huggingface.co/decapoda-research/llama-13b-hf-int4/resolve/main/llama-13b-4bit.pt\n",
    "LLAMA_13B_4BIT_CHECKPOINT_PATH = './llama-13b-4bit.pt'\n",
    "\n",
    "GPTJ_6B_MODEL_PATH = 'EleutherAI/gpt-j-6B'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and load fine-tuned LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb6d0be61b6496f84ee1520b4af9249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# model = load_llama_model_4bit_low_ram(LLAMA_13B_MODEL_PATH, LLAMA_13B_4BIT_CHECKPOINT_PATH, half=True)\n",
    "# model = PeftModel.from_pretrained(model, './loras/llama_13B_4bit_hatespeech_classification/', device_map={'': 0})\n",
    "# tokenizer = transformers.LlamaTokenizer.from_pretrained(\n",
    "#     \"decapoda-research/llama-13b-hf\", add_eos_token=True\n",
    "# )\n",
    "# model = model.eval()\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA_7B_MODEL_PATH, load_in_8bit=True, device_map={'': 0}, torch_dtype=torch.float16)\n",
    "lora_model = PeftModel.from_pretrained(model, './loras/llama_7B_8bit_hatespeech_classification', device_map={'': 0})\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\", add_eos_token=True\n",
    ")\n",
    "lora_model = model.eval()\n",
    "# model = torch.compile(model)\n",
    "\n",
    "\n",
    "# model = GPTJForCausalLM.from_pretrained(GPTJ_6B_MODEL_PATH, load_in_8bit=True, device_map={'': 0}, torch_dtype=torch.float16)\n",
    "# model = PeftModel.from_pretrained(model, './loras/gptj_6B_8bit_hatespeech_classification/', device_map={'': 0})\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     GPTJ_6B_MODEL_PATH, add_eos_token=True\n",
    "# )\n",
    "# model = model.eval()\n",
    "# model = torch.compile(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: hate, neutral, offensive\n",
      "\n",
      "Message: This is the great weather\n",
      "\n",
      "Category: neutral\n"
     ]
    }
   ],
   "source": [
    "# Test sample\n",
    "prompt = '''Classify the following messages into one of the following categories: hate, neutral, offensive\n",
    "\n",
    "Message: This is the great weather\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v[:, :-1].to('cuda') for k, v in sample.items()}\n",
    "gen_tokens = lora_model.generate(**sample, \n",
    "               do_sample=True, \n",
    "                temperature=0.2,\n",
    "                top_p=0.75,\n",
    "                top_k=40,\n",
    "                num_beams=4,\n",
    "               max_length=(sample['input_ids'].shape[-1]) + 1)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side effects\n",
    "The instruction based finetuning has intresting effects.  \n",
    "The model has trained its attention and now it can be used for classification of unseen labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Prime minister said no agreement had yet been made between the UK and the European Union.\n",
      "\n",
      "Category: Politics\n"
     ]
    }
   ],
   "source": [
    "# Let's try to detect bbc news topic\n",
    "\n",
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Prime minister said no agreement had yet been made between the UK and the European Union.\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v[:,:-1].to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               temperature=0.2, \n",
    "               do_sample=True, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 2)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Netflix cuts prices for subscribers in more than 30 countries\n",
      "\n",
      "Category: Business\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try to detect bbc news topic\n",
    "\n",
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Netflix cuts prices for subscribers in more than 30 countries\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v[:,:-1].to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               temperature=0.2,\n",
    "               do_sample=False, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 2)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Real Madrid's title hopes suffered a further setback after being held by 10-man rivals Atletico Madrid at the Bernabeu.\n",
      "\n",
      "Category: Sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try to detect bbc news topic\n",
    "\n",
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Real Madrid's title hopes suffered a further setback after being held by 10-man rivals Atletico Madrid at the Bernabeu.\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v[:,:-1].to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               temperature=0.1,\n",
    "               do_sample=False, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 2)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/vetka/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dfaf579c64405cbc7f6f19d093525c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47713</th>\n",
       "      <td>Politics</td>\n",
       "      <td>It's undetermined whether the FBI or the Justi...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43809</th>\n",
       "      <td>Politics</td>\n",
       "      <td>The West Virginia senator's unwillingness to b...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19830</th>\n",
       "      <td>Food</td>\n",
       "      <td>From fancy Spam crisps to fatty Spam sandwiche...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38873</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Students walked out in protest, and say they'l...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56862</th>\n",
       "      <td>Style</td>\n",
       "      <td>The University of Alabama is praised for its p...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description  \\\n",
       "47713  Politics  It's undetermined whether the FBI or the Justi...   \n",
       "43809  Politics  The West Virginia senator's unwillingness to b...   \n",
       "19830      Food  From fancy Spam crisps to fatty Spam sandwiche...   \n",
       "38873  Politics  Students walked out in protest, and say they'l...   \n",
       "56862     Style  The University of Alabama is praised for its p...   \n",
       "\n",
       "                                                  prompt  \n",
       "47713   Classify the following messages into one of t...  \n",
       "43809   Classify the following messages into one of t...  \n",
       "19830   Classify the following messages into one of t...  \n",
       "38873   Classify the following messages into one of t...  \n",
       "56862   Classify the following messages into one of t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset = load_dataset('heegyu/news-category-balanced-top10')\n",
    "\n",
    "\n",
    "\n",
    "to_replace = {'BUSINESS': 'Business', 'ENTERTAINMENT': 'Entertainment', 'FOOD & DRINK': 'Food', 'PARENTING': 'Parenting', 'POLITICS': 'Politics', 'STYLE & BEAUTY': 'Style', 'TRAVEL': 'Travel'}\n",
    "\n",
    "news_data = pd.DataFrame(news_dataset['train'])[['category', 'short_description']]\n",
    "news_data = news_data[news_data['category'].isin(to_replace)].sample(100, random_state=22)\n",
    "news_data['category'] = news_data['category'].replace(to_replace)\n",
    "\n",
    "news_categories = news_data['category'].unique()\n",
    "\n",
    "def create_instruction_prompt(text, all_labels):\n",
    "    prompt =  f''' Classify the following messages into one of the following categories: {', '.join(all_labels)}\n",
    "\n",
    "Message: {text}\n",
    "\n",
    "Category:'''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "news_data['prompt'] = news_data['short_description'].apply(lambda x: create_instruction_prompt(x[:150], news_categories))\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:35<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "generations = []\n",
    "for prompt in tqdm(news_data['prompt']):\n",
    "    with torch.no_grad():\n",
    "        sample = tokenizer(prompt, return_tensors='pt')\n",
    "        sample = {k: v[:, :-1].to('cuda') for k, v in sample.items()}\n",
    "        sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "        gen_tokens = model.generate(**sample, \n",
    "                    do_sample=True,\n",
    "                    temperature=0.4,\n",
    "                    top_p=0.75,\n",
    "                    top_k=40,\n",
    "                    num_beams=4,\n",
    "                    max_new_tokens=5)\n",
    "        generations.append(tokenizer.decode(gen_tokens[0][sample['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS CATEGORIZING ACCURACY: 0.54\n"
     ]
    }
   ],
   "source": [
    "def gen_accuracy(true_labels, gens):\n",
    "    total = len(true_labels)\n",
    "    correct = 0\n",
    "    for i in range(total):\n",
    "        len_true = len(true_labels[i])\n",
    "        if true_labels[i].lower() == gens[i].strip()[:len_true].lower():\n",
    "            correct += 1\n",
    "    return round(correct / total, 3)\n",
    "        \n",
    "    \n",
    "print(f\"NEWS CATEGORIZING ACCURACY: {gen_accuracy(list(news_data['category']), generations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
