{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "val_df = pd.read_csv('./data/val.csv')\n",
    "\n",
    "\n",
    "# We can create prompt in 2 ways: instruction or raw text\n",
    "# For instruction propmpt we will use a human understandable textual instruction.\n",
    "# For raw prompt we will use raw text with special separator between propmpt and completion\n",
    "\n",
    "def create_instruction_prompt(text, all_labels):\n",
    "    prompt =  f''' Classify the following messages into one of the following categories: {','.join(all_labels)}\n",
    "\n",
    "Message: {text}\n",
    "\n",
    "Category:'''\n",
    "    return prompt\n",
    "\n",
    "def create_raw_prompt(text):\n",
    "    prompt =  f'''{text} /n/n###/n/n'''\n",
    "    return prompt\n",
    "\n",
    "# For classification task we need 1 token completion. The completion token must be in model vocabulary. \n",
    "# GPT tokenization required completion tokens started with whitespace.\n",
    "\n",
    "train_df['completion'] = train_df['label'].apply(lambda x: ' '+ x)\n",
    "val_df['completion'] = val_df['label'].apply(lambda x: ' ' + x)\n",
    "\n",
    "# instruction based prompt\n",
    "all_labels = set(train_df['completion'].unique())\n",
    "train_df['prompt'] = train_df['text'].apply(lambda x: create_instruction_prompt(x, all_labels))\n",
    "val_df['prompt'] = val_df['text'].apply(lambda x: create_instruction_prompt(x, all_labels))\n",
    "\n",
    "# # raw text based prompt\n",
    "# train_df['completion'] = train_df['text'].apply(create_raw_prompt)\n",
    "# val_df['completion'] = val_df['text'].apply(create_raw_prompt)\n",
    "\n",
    "train_df[['prompt', 'completion']].to_json(\"./data/train_hatespeech.jsonl\", orient='records', lines=True)\n",
    "val_df[['prompt', 'completion']].to_json(\"./data/val_hatespeech.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminal\n",
    "# Use OpenAI API key https://platform.openai.com/account/api-keys\n",
    "\n",
    "# openai -k YOUR_API_KEY api fine_tunes.create -t ./data/train_hatespeech.jsonl -v ./data/val_hatespeech.jsonl -m ada --compute_classification_metrics --classification_n_classes 3 --n_epochs 4\n",
    "# openai -k YOUR_API_KEY api fine_tunes.create -t ./data/train_hatespeech.jsonl -v ./data/val_hatespeech.jsonl -m davinci --compute_classification_metrics --classification_n_classes 3 --n_epochs 4\n",
    "# openai wandb sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.wandb.ai/links/vetka925/tgmn8w11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of zero-shot ChatGPT (gpt-3.5-turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.10/site-packages (0.26.5)\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.0-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m238.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in ./venv/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.26.5\n",
      "    Uninstalling openai-0.26.5:\n",
      "      Successfully uninstalled openai-0.26.5\n",
      "Successfully installed openai-0.27.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('./data/val.csv')\n",
    "\n",
    "\n",
    "def create_instruction_prompt(text, all_labels):\n",
    "    prompt =  f''' Classify the following messages into one of the following categories: {', '.join(all_labels)}\n",
    "\n",
    "Message: {text}\n",
    "\n",
    "Category:'''\n",
    "    return prompt\n",
    "\n",
    "all_labels = set(val_df['label'].unique())\n",
    "\n",
    "val_df['prompt'] = val_df['text'].apply(lambda x: create_instruction_prompt(x, all_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a748536d66b4cc081a181c6f14c6b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "result = []\n",
    "for prompt in tqdm(val_df['prompt']):\n",
    "    time.sleep(0.1)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a hate speech, offensive language classifier.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    result.append(response['choices'][0]['message']['content'].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hate', 'offensive', 'hate.', 'hate', 'neutral', 'neutral.', 'offensive', 'hate.', 'hate', 'offensive/hate.']\n"
     ]
    }
   ],
   "source": [
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HATESPEECH DETECTION ACCURACY: 0.71\n"
     ]
    }
   ],
   "source": [
    "def gen_accuracy(true_labels, gens):\n",
    "    total = len(true_labels)\n",
    "    correct = 0\n",
    "    for i in range(total):\n",
    "        len_true = len(true_labels[i])\n",
    "        if true_labels[i].lower() == gens[i].strip()[:len_true].lower():\n",
    "            correct += 1\n",
    "    return round(correct / total, 3)\n",
    "        \n",
    "    \n",
    "print(f\"HATESPEECH DETECTION ACCURACY: {gen_accuracy(list(val_df['label']), result)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate news categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/vetka/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89491128c31d4920ab8e37c4debf9a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47713</th>\n",
       "      <td>Politics</td>\n",
       "      <td>It's undetermined whether the FBI or the Justi...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43809</th>\n",
       "      <td>Politics</td>\n",
       "      <td>The West Virginia senator's unwillingness to b...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19830</th>\n",
       "      <td>Food</td>\n",
       "      <td>From fancy Spam crisps to fatty Spam sandwiche...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38873</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Students walked out in protest, and say they'l...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56862</th>\n",
       "      <td>Style</td>\n",
       "      <td>The University of Alabama is praised for its p...</td>\n",
       "      <td>Classify the following messages into one of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                  short_description  \\\n",
       "47713  Politics  It's undetermined whether the FBI or the Justi...   \n",
       "43809  Politics  The West Virginia senator's unwillingness to b...   \n",
       "19830      Food  From fancy Spam crisps to fatty Spam sandwiche...   \n",
       "38873  Politics  Students walked out in protest, and say they'l...   \n",
       "56862     Style  The University of Alabama is praised for its p...   \n",
       "\n",
       "                                                  prompt  \n",
       "47713   Classify the following messages into one of t...  \n",
       "43809   Classify the following messages into one of t...  \n",
       "19830   Classify the following messages into one of t...  \n",
       "38873   Classify the following messages into one of t...  \n",
       "56862   Classify the following messages into one of t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "news_dataset = load_dataset('heegyu/news-category-balanced-top10')\n",
    "\n",
    "\n",
    "\n",
    "to_replace = {'BUSINESS': 'Business', 'ENTERTAINMENT': 'Entertainment', 'FOOD & DRINK': 'Food', 'PARENTING': 'Parenting', 'POLITICS': 'Politics', 'STYLE & BEAUTY': 'Style', 'TRAVEL': 'Travel'}\n",
    "\n",
    "news_data = pd.DataFrame(news_dataset['train'])[['category', 'short_description']]\n",
    "news_data = news_data[news_data['category'].isin(to_replace)].sample(100, random_state=22)\n",
    "news_data['category'] = news_data['category'].replace(to_replace)\n",
    "\n",
    "news_categories = news_data['category'].unique()\n",
    "\n",
    "def create_instruction_prompt(text, all_labels):\n",
    "    prompt =  f''' Classify the following messages into one of the following categories: {', '.join(all_labels)}\n",
    "\n",
    "Message: {text}\n",
    "\n",
    "Category:'''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "news_data['prompt'] = news_data['short_description'].apply(lambda x: create_instruction_prompt(x[:150], news_categories))\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "result = []\n",
    "for prompt in tqdm(news_data['prompt']):\n",
    "    time.sleep(0.1)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a news classifier.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    result.append(response['choices'][0]['message']['content'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS CATEGORIZING ACCURACY: 0.71\n"
     ]
    }
   ],
   "source": [
    "def gen_accuracy(true_labels, gens):\n",
    "    total = len(true_labels)\n",
    "    correct = 0\n",
    "    for i in range(total):\n",
    "        len_true = len(true_labels[i])\n",
    "        if true_labels[i].lower() == gens[i].strip()[:len_true].lower():\n",
    "            correct += 1\n",
    "    return round(correct / total, 3)\n",
    "        \n",
    "    \n",
    "print(f\"NEWS CATEGORIZING ACCURACY: {gen_accuracy(list(news_data['category']), result)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "855cc711d80c8d878070baad7d5f36be1934899c6a0e360cd07c7a1deca02102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
