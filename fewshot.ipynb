{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from custom_datasets import PromptDataset\n",
    "\n",
    "\n",
    "LLAMA_7B_MODEL_PATH = 'decapoda-research/llama-7b-hf'\n",
    "# !wget https://huggingface.co/decapoda-research/llama-7b-hf-int4/resolve/main/llama-7b-4bit.pt\n",
    "LLAMA_7B_4BIT_CHECKPOINT_PATH = './llama-7b-4bit.pt'\n",
    "\n",
    "LLAMA_13B_MODEL_PATH = 'decapoda-research/llama-13b-hf'\n",
    "# !wget https://huggingface.co/decapoda-research/llama-13b-hf-int4/resolve/main/llama-13b-4bit.pt\n",
    "LLAMA_13B_4BIT_CHECKPOINT_PATH = './llama-13b-4bit.pt'\n",
    "\n",
    "GPTJ_6B_MODEL_PATH = 'EleutherAI/gpt-j-6B'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/vetka/miniconda3/envs/transformers did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('VSCODE_WSL_EXT_LOCATION/up')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/vetka/miniconda3/envs/transformers/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0d2c085cf344fc99d5a6432121a537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(LLAMA_7B_MODEL_PATH)\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA_7B_MODEL_PATH, load_in_8bit=True, device_map={'': 0}, torch_dtype=torch.float16)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, GPT! How are you? - I'm fine, thanks.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "text = 'Hi, GPT! How are you? -'\n",
    "sample = tokenizer(text, return_tensors='pt')\n",
    "sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample,\n",
    "                            temperature=0.2,\n",
    "                            do_sample=True,  \n",
    "                            max_length=20)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fewshot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try Hate Speech and Offensive Language Detection with this prompt\n",
    "\n",
    "def create_prompt(text):\n",
    "        prompt = f'''Classify the following messages into one of the following categories: neutral, hate, offensive\n",
    "\n",
    "Message: I could go for a brownie right now\n",
    "Category: neutral\n",
    "/n/n###/n/n\n",
    "Message: What these bitches want from a nigga?, like on some DMX shit\n",
    "Category: hate\n",
    "/n/n###/n/n\n",
    "Message: So says the one who fills an empty hole inside herself by worshiping a royal. Who 4 all u kno may be a raging snotty bitch\n",
    "Category: off\n",
    "/n/n###/n/n\n",
    "Message: {text}\n",
    "Category:'''\n",
    "        return prompt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tokens and token ids for labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21104: '▁neutral', 26277: '▁hate', 1283: '▁off'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenizer.tokenize('neutral hate off')\n",
    "label_ids = tokenizer.convert_tokens_to_ids(labels)\n",
    "id2label = dict(zip(label_ids, labels))\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: neutral, hate, offensive\n",
      "\n",
      "Message: I could go for a brownie right now\n",
      "Category: neutral\n",
      "/n/n###/n/n\n",
      "Message: What these bitches want from a nigga?, like on some DMX shit\n",
      "Category: hate\n",
      "/n/n###/n/n\n",
      "Message: So says the one who fills an empty hole inside herself by worshiping a royal. Who 4 all u kno may be a raging snotty bitch\n",
      "Category: off\n",
      "/n/n###/n/n\n",
      "Message: Toda will be rainy and cloudy. Im going to take an umbrella.\n",
      "Category: neutral\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(**sample,  \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 1)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation on val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4e75e3ec2e498abf62d36bb017aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_df = pd.read_csv('./data/val.csv')\n",
    "\n",
    "val_df['prompt'] = val_df['text'].apply(create_prompt)\n",
    "val_df['label'] = val_df['label'].apply(lambda x: ' ' + x)\n",
    "\n",
    "\n",
    "predicted_tokens = []\n",
    "for i, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    sample = tokenizer(row['prompt'], return_tensors='pt')\n",
    "    sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "    # We will take logits from raw output of model\n",
    "    out = model(**sample)\n",
    "    # Take token from labels with max logit\n",
    "    max_ind_label = torch.argmax(out.logits[:, -1].flatten()[label_ids]).detach().cpu()\n",
    "    predicted_token = tokenizer.decode(label_ids[max_ind_label])\n",
    "    predicted_tokens.append(predicted_token)\n",
    "    del out\n",
    "    del sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Fewshot predictions: 0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Accuracy of Fewshot predictions: {accuracy_score(val_df['label'], predicted_tokens)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check other classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Prime minister said no agreement had yet been made between the UK and the European Union.\n",
      "\n",
      "Category: Polit\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Prime minister said no agreement had yet been made between the UK and the European Union.\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 1)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Netflix cuts prices for subscribers in more than 30 countries\n",
      "\n",
      "Category: Business\n"
     ]
    }
   ],
   "source": [
    "# Let's try to detect bbc news topic\n",
    "\n",
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Netflix cuts prices for subscribers in more than 30 countries\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 1)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
      "\n",
      "Message: Real Madrid's title hopes suffered a further setback after being held by 10-man rivals Atletico Madrid at the Bernabeu.\n",
      "\n",
      "Category: Sports\n"
     ]
    }
   ],
   "source": [
    "# Let's try to detect bbc news topic\n",
    "\n",
    "prompt = '''Classify the following messages into one of the following categories: Politics, Sports, Business, Space, Tech, Social\n",
    "\n",
    "Message: Real Madrid's title hopes suffered a further setback after being held by 10-man rivals Atletico Madrid at the Bernabeu.\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sample = tokenizer(prompt, return_tensors='pt')\n",
    "sample = {k: v.to('cuda') for k, v in sample.items()}\n",
    "\n",
    "gen_tokens = model.generate(**sample, \n",
    "               temperature=0.1,\n",
    "               do_sample=False, \n",
    "               max_length=(sample['input_ids'].shape[-1]) + 1)\n",
    "print(tokenizer.decode(gen_tokens[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cocnlusion\n",
    " Fine-tuning is much better than few-shot predictions of hatespeech and raw predictions of other tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "855cc711d80c8d878070baad7d5f36be1934899c6a0e360cd07c7a1deca02102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
